{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2ndPTB.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yBKDakdooDxw","colab_type":"code","outputId":"629b1886-e268-488d-ea64-2ff8205ba4ec","executionInfo":{"status":"ok","timestamp":1544616438105,"user_tz":-120,"elapsed":4153,"user":{"displayName":"Stav Bar-Sheshet","photoUrl":"","userId":"04728265830245650668"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["!git clone https://github.com/wojzaremba/lstm.git\n","!ls \"lstm/data\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'lstm'...\n","remote: Enumerating objects: 53, done.\u001b[K\n","remote: Total 53 (delta 0), reused 0 (delta 0), pack-reused 53\u001b[K\n","Unpacking objects: 100% (53/53), done.\n","ptb.test.txt  ptb.train.txt  ptb.valid.txt\n"],"name":"stdout"}]},{"metadata":{"id":"bFUWsjIAa-jl","colab_type":"code","outputId":"6bf3bc01-8a79-43e7-ba75-819411488fa4","executionInfo":{"status":"error","timestamp":1544616758011,"user_tz":-120,"elapsed":319606,"user":{"displayName":"Stav Bar-Sheshet","photoUrl":"","userId":"04728265830245650668"}},"colab":{"base_uri":"https://localhost:8080/","height":11553}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import argparse\n","import os\n","import sys\n","import time\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow.contrib.cudnn_rnn.python.layers import cudnn_rnn\n","from tensorflow.contrib.eager.python import tfe\n","\n","layers = tf.keras.layers\n","\n","\n","class Embedding(layers.Layer):\n","  \"\"\"An Embedding layer.\"\"\"\n","\n","  def __init__(self, vocab_size, embedding_dim, **kwargs):\n","    super(Embedding, self).__init__(**kwargs)\n","    self.vocab_size = vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","  def build(self, _):\n","    self.embedding = self.add_variable(\n","        \"embedding_kernel\",\n","        shape=[self.vocab_size, self.embedding_dim],\n","        dtype=tf.float32,\n","        initializer=tf.random_uniform_initializer(-0.1, 0.1),\n","        trainable=True)\n","\n","  def call(self, x):\n","    return tf.nn.embedding_lookup(self.embedding, x)\n","\n","\n","class PTBModel(tf.keras.Model):\n","  \n","  def __init__(self,\n","               vocab_size,\n","               embedding_dim,\n","               hidden_dim,\n","               num_layers,\n","               dropout_ratio,\n","               use_cudnn_rnn=True):\n","    super(PTBModel, self).__init__()\n","\n","    self.keep_ratio = 1 - dropout_ratio\n","    self.use_cudnn_rnn = use_cudnn_rnn\n","    self.embedding = Embedding(vocab_size, embedding_dim)\n","\n","    self.rnn = cudnn_rnn.CudnnLSTM(\n","          num_layers, hidden_dim, dropout=dropout_ratio)\n","\n","    self.linear = layers.Dense(\n","        vocab_size, kernel_initializer=tf.random_uniform_initializer(-0.1, 0.1))\n","    self._output_shape = [-1, embedding_dim]\n","\n","  def call(self, input_seq, training):\n","    y = self.embedding(input_seq)\n","    if training:\n","      y = tf.nn.dropout(y, self.keep_ratio)\n","    y = self.rnn(y, training=training)[0]\n","    return self.linear(tf.reshape(y, self._output_shape))\n","\n","\n","def clip_gradients(grads_and_vars, clip_ratio):\n","  gradients, variables = zip(*grads_and_vars)\n","  clipped, _ = tf.clip_by_global_norm(gradients, clip_ratio)\n","  return zip(clipped, variables)\n","\n","\n","def loss_fn(model, inputs, targets, training):\n","  labels = tf.reshape(targets, [-1])\n","  outputs = model(inputs, training=training)\n","  return tf.exp(tf.reduce_mean(\n","      tf.nn.sparse_softmax_cross_entropy_with_logits(\n","          labels=labels, logits=outputs)))\n","\n","\n","def _divide_into_batches(data, batch_size):\n","  \"\"\"Convert a sequence to a batch of sequences.\"\"\"\n","  nbatch = data.shape[0] // batch_size\n","  data = data[:nbatch * batch_size]\n","  data = data.reshape(batch_size, -1).transpose()\n","  return data\n","\n","\n","def _get_batch(data, i, seq_len):\n","  slen = min(seq_len, data.shape[0] - 1 - i)\n","  inputs = data[i:i + slen, :]\n","  target = data[i + 1:i + 1 + slen, :]\n","  return tf.constant(inputs), tf.constant(target)\n","\n","\n","def evaluate(model, data):\n","  \"\"\"evaluate an epoch.\"\"\"\n","  total_loss = 0.0\n","  total_batches = 0\n","  start = time.time()\n","  for _, i in enumerate(range(0, data.shape[0] - 1, FLAGS.seq_len)):\n","    inp, target = _get_batch(data, i, FLAGS.seq_len)\n","    loss = loss_fn(model, inp, target, training=False)\n","    total_loss += loss.numpy()\n","    total_batches += 1\n","  time_in_ms = (time.time() - start) * 1000\n","  sys.stderr.write(\"eval loss %.2f (eval took %d ms)\\n\" %\n","                   (total_loss / total_batches, time_in_ms))\n","  return total_loss\n","\n","\n","def train(model, optimizer, train_data, sequence_length, clip_ratio):\n","  \"\"\"training an epoch.\"\"\"\n","\n","  def model_loss(inputs, targets):\n","    return loss_fn(model, inputs, targets, training=True)\n","\n","  grads = tfe.implicit_gradients(model_loss)\n","\n","  total_time = 0\n","  for batch, i in enumerate(range(0, train_data.shape[0] - 1, sequence_length)):\n","    train_seq, train_target = _get_batch(train_data, i, sequence_length)\n","    start = time.time()\n","    optimizer.apply_gradients(\n","        clip_gradients(grads(train_seq, train_target), clip_ratio))\n","    total_time += (time.time() - start)\n","    if batch % 10 == 0:\n","      time_in_ms = (total_time * 1000) / (batch + 1)\n","      sys.stderr.write(\"batch %d: training loss %.2f, avg step time %d ms\\n\" %\n","                       (batch, model_loss(train_seq, train_target).numpy(),\n","                        time_in_ms))\n","\n","\n","class Datasets(object):\n","  \"\"\"Processed form of the Penn Treebank dataset.\"\"\"\n","\n","  def __init__(self, path):\n","    \"\"\"Load the Penn Treebank dataset.\"\"\"\n","\n","    self.word2idx = {}  # string -> integer id\n","    self.idx2word = []  # integer id -> word string\n","    # Files represented as a list of integer ids (as opposed to list of string\n","    # words).\n","    self.train = self.tokenize(os.path.join(path, \"ptb.train.txt\"))\n","    self.valid = self.tokenize(os.path.join(path, \"ptb.valid.txt\"))\n","\n","  def vocab_size(self):\n","    return len(self.idx2word)\n","\n","  def add(self, word):\n","    if word not in self.word2idx:\n","      self.idx2word.append(word)\n","      self.word2idx[word] = len(self.idx2word) - 1\n","\n","  def tokenize(self, path):\n","    \"\"\"Read text file in path and return a list of integer token ids.\"\"\"\n","    tokens = 0\n","    with tf.gfile.Open(path, \"r\") as f:\n","      for line in f:\n","        words = line.split() + [\"<eos>\"]\n","        tokens += len(words)\n","        for word in words:\n","          self.add(word)\n","\n","    # Tokenize file content\n","    with tf.gfile.Open(path, \"r\") as f:\n","      ids = np.zeros(tokens).astype(np.int64)\n","      token = 0\n","      for line in f:\n","        words = line.split() + [\"<eos>\"]\n","        for word in words:\n","          ids[token] = self.word2idx[word]\n","          token += 1\n","    \n","    return ids\n","\n","\n","def small_model():\n","  \"\"\"Returns a PTBModel with a 'small' configuration.\"\"\"\n","  return PTBModel(\n","      vocab_size=10000,\n","      embedding_dim=200,\n","      hidden_dim=200,\n","      num_layers=2,\n","      dropout_ratio=0.,\n","      use_cudnn_rnn=True)\n","\n","\n","def main(_):\n","  tf.enable_eager_execution()\n","\n","  if not FLAGS.data_path:\n","    raise ValueError(\"Must specify --data-path\")\n","  corpus = Datasets(FLAGS.data_path)\n","  train_data = _divide_into_batches(corpus.train, FLAGS.batch_size)\n","  eval_data = _divide_into_batches(corpus.valid, 10)\n","\n","  with tf.device(\"/device:GPU:0\"):\n","    learning_rate = tf.Variable(20.0, name=\"learning_rate\")\n","    model = PTBModel(corpus.vocab_size(), FLAGS.embedding_dim,\n","                     FLAGS.hidden_dim, FLAGS.num_layers, FLAGS.dropout,\n","                     True)\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    checkpoint = tf.train.Checkpoint(\n","        learning_rate=learning_rate, model=model,\n","        optimizer=optimizer)\n","    checkpoint.restore(tf.train.latest_checkpoint(FLAGS.logdir))\n","    sys.stderr.write(\"learning_rate=%f\\n\" % learning_rate.numpy())\n","\n","    best_loss = None\n","    for _ in range(FLAGS.epoch):\n","      train(model, optimizer, train_data, FLAGS.seq_len, FLAGS.clip)\n","      eval_loss = evaluate(model, eval_data)\n","      if not best_loss or eval_loss < best_loss:\n","        if FLAGS.logdir:\n","          checkpoint.save(os.path.join(FLAGS.logdir, \"ckpt\"))\n","        best_loss = eval_loss\n","      else:\n","        learning_rate.assign(learning_rate / 4.0)\n","        sys.stderr.write(\"eval_loss did not reduce in this epoch, \"\n","                         \"changing learning rate to %f for the next epoch\\n\" %\n","                         learning_rate.numpy())\n","\n","\n","if __name__ == \"__main__\":\n","  parser = argparse.ArgumentParser()\n","  parser.add_argument(\n","      \"--data-path\",\n","      type=str,\n","      default=\"lstm/data\",\n","      help=\"Data directory of the Penn Treebank dataset from \"\n","      \"http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\")\n","  parser.add_argument(\n","      \"--logdir\", type=str, default=\"\", help=\"Directory for checkpoint.\")\n","  parser.add_argument(\"--epoch\", type=int, default=5, help=\"Number of epochs.\")\n","  parser.add_argument(\"--batch-size\", type=int, default=20, help=\"Batch size.\")\n","  parser.add_argument(\n","      \"--seq-len\", type=int, default=35, help=\"Sequence length.\")\n","  parser.add_argument(\n","      \"--embedding-dim\", type=int, default=200, help=\"Embedding dimension.\")\n","  parser.add_argument(\n","      \"--hidden-dim\", type=int, default=200, help=\"Hidden layer dimension.\")\n","  parser.add_argument(\n","      \"--num-layers\", type=int, default=2, help=\"Number of RNN layers.\")\n","  parser.add_argument(\n","      \"--dropout\", type=float, default=0.0, help=\"Drop out ratio.\")\n","  parser.add_argument(\n","      \"--clip\", type=float, default=0.25, help=\"Gradient clipping ratio.\")\n","\n","  FLAGS, unparsed = parser.parse_known_args()\n","  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["learning_rate=20.000000\n","batch 0: training loss 5342.12, avg step time 2084 ms\n","batch 10: training loss 3044.10, avg step time 229 ms\n","batch 20: training loss 4638.15, avg step time 140 ms\n","batch 30: training loss 1025.68, avg step time 111 ms\n","batch 40: training loss 1051.51, avg step time 94 ms\n","batch 50: training loss 1009.70, avg step time 84 ms\n","batch 60: training loss 926.50, avg step time 76 ms\n","batch 70: training loss 726.72, avg step time 71 ms\n","batch 80: training loss 611.51, avg step time 67 ms\n","batch 90: training loss 771.31, avg step time 65 ms\n","batch 100: training loss 604.07, avg step time 62 ms\n","batch 110: training loss 719.60, avg step time 60 ms\n","batch 120: training loss 736.13, avg step time 58 ms\n","batch 130: training loss 592.86, avg step time 57 ms\n","batch 140: training loss 450.92, avg step time 56 ms\n","batch 150: training loss 662.33, avg step time 55 ms\n","batch 160: training loss 448.98, avg step time 55 ms\n","batch 170: training loss 602.61, avg step time 54 ms\n","batch 180: training loss 518.68, avg step time 53 ms\n","batch 190: training loss 509.79, avg step time 52 ms\n","batch 200: training loss 398.80, avg step time 52 ms\n","batch 210: training loss 408.20, avg step time 51 ms\n","batch 220: training loss 443.86, avg step time 51 ms\n","batch 230: training loss 333.49, avg step time 50 ms\n","batch 240: training loss 371.31, avg step time 50 ms\n","batch 250: training loss 369.38, avg step time 49 ms\n","batch 260: training loss 350.57, avg step time 49 ms\n","batch 270: training loss 491.66, avg step time 49 ms\n","batch 280: training loss 306.28, avg step time 48 ms\n","batch 290: training loss 371.76, avg step time 48 ms\n","batch 300: training loss 271.37, avg step time 48 ms\n","batch 310: training loss 308.89, avg step time 48 ms\n","batch 320: training loss 215.40, avg step time 48 ms\n","batch 330: training loss 270.69, avg step time 47 ms\n","batch 340: training loss 336.96, avg step time 47 ms\n","batch 350: training loss 251.17, avg step time 47 ms\n","batch 360: training loss 224.63, avg step time 47 ms\n","batch 370: training loss 409.98, avg step time 47 ms\n","batch 380: training loss 176.12, avg step time 47 ms\n","batch 390: training loss 279.69, avg step time 47 ms\n","batch 400: training loss 288.82, avg step time 46 ms\n","batch 410: training loss 234.85, avg step time 46 ms\n","batch 420: training loss 317.80, avg step time 46 ms\n","batch 430: training loss 193.54, avg step time 46 ms\n","batch 440: training loss 175.48, avg step time 46 ms\n","batch 450: training loss 213.43, avg step time 46 ms\n","batch 460: training loss 265.50, avg step time 45 ms\n","batch 470: training loss 216.11, avg step time 45 ms\n","batch 480: training loss 233.87, avg step time 45 ms\n","batch 490: training loss 263.68, avg step time 45 ms\n","batch 500: training loss 222.04, avg step time 45 ms\n","batch 510: training loss 186.41, avg step time 45 ms\n","batch 520: training loss 178.85, avg step time 45 ms\n","batch 530: training loss 170.41, avg step time 45 ms\n","batch 540: training loss 251.61, avg step time 44 ms\n","batch 550: training loss 152.75, avg step time 44 ms\n","batch 560: training loss 171.13, avg step time 44 ms\n","batch 570: training loss 147.28, avg step time 44 ms\n","batch 580: training loss 191.73, avg step time 44 ms\n","batch 590: training loss 219.05, avg step time 44 ms\n","batch 600: training loss 210.57, avg step time 44 ms\n","batch 610: training loss 183.01, avg step time 44 ms\n","batch 620: training loss 169.07, avg step time 44 ms\n","batch 630: training loss 210.19, avg step time 44 ms\n","batch 640: training loss 151.92, avg step time 44 ms\n","batch 650: training loss 146.85, avg step time 44 ms\n","batch 660: training loss 142.65, avg step time 44 ms\n","batch 670: training loss 112.23, avg step time 44 ms\n","batch 680: training loss 176.39, avg step time 44 ms\n","batch 690: training loss 158.62, avg step time 44 ms\n","batch 700: training loss 158.35, avg step time 44 ms\n","batch 710: training loss 140.67, avg step time 44 ms\n","batch 720: training loss 120.57, avg step time 44 ms\n","batch 730: training loss 116.95, avg step time 43 ms\n","batch 740: training loss 132.43, avg step time 43 ms\n","batch 750: training loss 139.19, avg step time 43 ms\n","batch 760: training loss 128.24, avg step time 43 ms\n","batch 770: training loss 142.46, avg step time 43 ms\n","batch 780: training loss 110.50, avg step time 43 ms\n","batch 790: training loss 134.60, avg step time 43 ms\n","batch 800: training loss 140.08, avg step time 43 ms\n","batch 810: training loss 133.20, avg step time 43 ms\n","batch 820: training loss 166.38, avg step time 43 ms\n","batch 830: training loss 134.57, avg step time 43 ms\n","batch 840: training loss 146.74, avg step time 43 ms\n","batch 850: training loss 121.77, avg step time 43 ms\n","batch 860: training loss 105.30, avg step time 43 ms\n","batch 870: training loss 90.59, avg step time 43 ms\n","batch 880: training loss 99.25, avg step time 43 ms\n","batch 890: training loss 177.85, avg step time 43 ms\n","batch 900: training loss 127.21, avg step time 43 ms\n","batch 910: training loss 164.16, avg step time 43 ms\n","batch 920: training loss 145.17, avg step time 43 ms\n","batch 930: training loss 221.22, avg step time 43 ms\n","batch 940: training loss 125.28, avg step time 43 ms\n","batch 950: training loss 173.76, avg step time 43 ms\n","batch 960: training loss 128.69, avg step time 43 ms\n","batch 970: training loss 130.42, avg step time 43 ms\n","batch 980: training loss 129.14, avg step time 43 ms\n","batch 990: training loss 140.32, avg step time 43 ms\n","batch 1000: training loss 122.30, avg step time 43 ms\n","batch 1010: training loss 149.72, avg step time 43 ms\n","batch 1020: training loss 108.77, avg step time 43 ms\n","batch 1030: training loss 120.75, avg step time 42 ms\n","batch 1040: training loss 88.76, avg step time 42 ms\n","batch 1050: training loss 118.10, avg step time 42 ms\n","batch 1060: training loss 88.43, avg step time 42 ms\n","batch 1070: training loss 76.41, avg step time 42 ms\n","batch 1080: training loss 99.04, avg step time 42 ms\n","batch 1090: training loss 126.80, avg step time 42 ms\n","batch 1100: training loss 129.27, avg step time 42 ms\n","batch 1110: training loss 128.00, avg step time 42 ms\n","batch 1120: training loss 113.40, avg step time 42 ms\n","batch 1130: training loss 99.32, avg step time 42 ms\n","batch 1140: training loss 149.12, avg step time 42 ms\n","batch 1150: training loss 68.96, avg step time 42 ms\n","batch 1160: training loss 94.01, avg step time 42 ms\n","batch 1170: training loss 116.25, avg step time 42 ms\n","batch 1180: training loss 83.53, avg step time 42 ms\n","batch 1190: training loss 94.62, avg step time 42 ms\n","batch 1200: training loss 101.85, avg step time 42 ms\n","batch 1210: training loss 80.69, avg step time 42 ms\n","batch 1220: training loss 112.54, avg step time 42 ms\n","batch 1230: training loss 97.66, avg step time 42 ms\n","batch 1240: training loss 119.76, avg step time 42 ms\n","batch 1250: training loss 96.41, avg step time 42 ms\n","batch 1260: training loss 94.58, avg step time 42 ms\n","batch 1270: training loss 88.52, avg step time 42 ms\n","batch 1280: training loss 122.00, avg step time 42 ms\n","batch 1290: training loss 154.67, avg step time 42 ms\n","batch 1300: training loss 121.97, avg step time 42 ms\n","batch 1310: training loss 100.22, avg step time 42 ms\n","batch 1320: training loss 135.31, avg step time 42 ms\n","eval loss 209.41 (eval took 3546 ms)\n","batch 0: training loss 155.59, avg step time 39 ms\n","batch 10: training loss 128.22, avg step time 39 ms\n","batch 20: training loss 131.22, avg step time 40 ms\n","batch 30: training loss 103.36, avg step time 40 ms\n","batch 40: training loss 127.85, avg step time 40 ms\n","batch 50: training loss 93.58, avg step time 40 ms\n","batch 60: training loss 117.15, avg step time 40 ms\n","batch 70: training loss 94.98, avg step time 40 ms\n","batch 80: training loss 83.35, avg step time 40 ms\n","batch 90: training loss 86.05, avg step time 40 ms\n","batch 100: training loss 72.20, avg step time 40 ms\n","batch 110: training loss 92.74, avg step time 40 ms\n","batch 120: training loss 87.22, avg step time 40 ms\n","batch 130: training loss 82.31, avg step time 40 ms\n","batch 140: training loss 101.56, avg step time 40 ms\n","batch 150: training loss 113.76, avg step time 40 ms\n","batch 160: training loss 102.37, avg step time 40 ms\n","batch 170: training loss 106.57, avg step time 40 ms\n","batch 180: training loss 106.02, avg step time 40 ms\n","batch 190: training loss 100.95, avg step time 40 ms\n","batch 200: training loss 117.60, avg step time 40 ms\n","batch 210: training loss 91.22, avg step time 40 ms\n","batch 220: training loss 98.27, avg step time 40 ms\n","batch 230: training loss 84.33, avg step time 40 ms\n","batch 240: training loss 83.14, avg step time 40 ms\n","batch 250: training loss 99.06, avg step time 41 ms\n","batch 260: training loss 103.44, avg step time 41 ms\n","batch 270: training loss 109.12, avg step time 41 ms\n","batch 280: training loss 77.89, avg step time 41 ms\n","batch 290: training loss 103.30, avg step time 40 ms\n","batch 300: training loss 74.94, avg step time 40 ms\n","batch 310: training loss 70.60, avg step time 40 ms\n","batch 320: training loss 58.16, avg step time 40 ms\n","batch 330: training loss 97.01, avg step time 40 ms\n","batch 340: training loss 138.43, avg step time 40 ms\n","batch 350: training loss 92.96, avg step time 40 ms\n","batch 360: training loss 73.59, avg step time 40 ms\n","batch 370: training loss 119.50, avg step time 40 ms\n","batch 380: training loss 56.58, avg step time 40 ms\n","batch 390: training loss 120.21, avg step time 40 ms\n","batch 400: training loss 89.03, avg step time 40 ms\n","batch 410: training loss 79.75, avg step time 40 ms\n","batch 420: training loss 110.02, avg step time 40 ms\n","batch 430: training loss 72.50, avg step time 40 ms\n","batch 440: training loss 70.03, avg step time 40 ms\n","batch 450: training loss 86.50, avg step time 40 ms\n","batch 460: training loss 100.48, avg step time 40 ms\n","batch 470: training loss 90.94, avg step time 40 ms\n","batch 480: training loss 106.84, avg step time 40 ms\n","batch 490: training loss 122.60, avg step time 40 ms\n","batch 500: training loss 107.95, avg step time 40 ms\n","batch 510: training loss 84.94, avg step time 40 ms\n","batch 520: training loss 82.57, avg step time 40 ms\n","batch 530: training loss 71.80, avg step time 40 ms\n","batch 540: training loss 112.76, avg step time 40 ms\n","batch 550: training loss 57.17, avg step time 40 ms\n","batch 560: training loss 72.43, avg step time 40 ms\n","batch 570: training loss 58.37, avg step time 40 ms\n","batch 580: training loss 91.50, avg step time 40 ms\n","batch 590: training loss 114.27, avg step time 40 ms\n","batch 600: training loss 99.39, avg step time 40 ms\n","batch 610: training loss 88.05, avg step time 40 ms\n","batch 620: training loss 71.28, avg step time 40 ms\n","batch 630: training loss 100.69, avg step time 40 ms\n","batch 640: training loss 74.39, avg step time 40 ms\n","batch 650: training loss 62.33, avg step time 40 ms\n","batch 660: training loss 63.75, avg step time 40 ms\n","batch 670: training loss 52.21, avg step time 40 ms\n","batch 680: training loss 82.82, avg step time 40 ms\n","batch 690: training loss 81.36, avg step time 40 ms\n","batch 700: training loss 80.75, avg step time 40 ms\n","batch 710: training loss 69.46, avg step time 40 ms\n","batch 720: training loss 64.23, avg step time 40 ms\n","batch 730: training loss 65.82, avg step time 40 ms\n","batch 740: training loss 63.31, avg step time 40 ms\n","batch 750: training loss 63.38, avg step time 40 ms\n","batch 760: training loss 66.08, avg step time 40 ms\n","batch 770: training loss 66.47, avg step time 40 ms\n","batch 780: training loss 53.69, avg step time 40 ms\n","batch 790: training loss 80.12, avg step time 40 ms\n","batch 800: training loss 78.62, avg step time 40 ms\n","batch 810: training loss 71.43, avg step time 40 ms\n","batch 820: training loss 89.67, avg step time 40 ms\n","batch 830: training loss 74.71, avg step time 40 ms\n","batch 840: training loss 79.11, avg step time 40 ms\n","batch 850: training loss 65.85, avg step time 40 ms\n","batch 860: training loss 53.81, avg step time 40 ms\n","batch 870: training loss 47.00, avg step time 40 ms\n","batch 880: training loss 56.87, avg step time 40 ms\n","batch 890: training loss 114.75, avg step time 40 ms\n","batch 900: training loss 72.89, avg step time 40 ms\n","batch 910: training loss 93.55, avg step time 40 ms\n","batch 920: training loss 82.75, avg step time 40 ms\n","batch 930: training loss 134.63, avg step time 40 ms\n","batch 940: training loss 67.36, avg step time 40 ms\n","batch 950: training loss 96.35, avg step time 40 ms\n","batch 960: training loss 75.12, avg step time 40 ms\n","batch 970: training loss 79.13, avg step time 40 ms\n","batch 980: training loss 76.60, avg step time 40 ms\n","batch 990: training loss 81.64, avg step time 40 ms\n","batch 1000: training loss 72.82, avg step time 40 ms\n","batch 1010: training loss 89.98, avg step time 40 ms\n","batch 1020: training loss 67.90, avg step time 40 ms\n","batch 1030: training loss 80.09, avg step time 40 ms\n","batch 1040: training loss 48.32, avg step time 40 ms\n","batch 1050: training loss 71.89, avg step time 40 ms\n","batch 1060: training loss 48.95, avg step time 40 ms\n","batch 1070: training loss 45.62, avg step time 40 ms\n","batch 1080: training loss 58.24, avg step time 40 ms\n","batch 1090: training loss 67.90, avg step time 40 ms\n","batch 1100: training loss 69.50, avg step time 40 ms\n","batch 1110: training loss 80.77, avg step time 40 ms\n","batch 1120: training loss 73.94, avg step time 40 ms\n","batch 1130: training loss 61.81, avg step time 40 ms\n","batch 1140: training loss 86.81, avg step time 40 ms\n","batch 1150: training loss 41.10, avg step time 40 ms\n","batch 1160: training loss 61.74, avg step time 40 ms\n","batch 1170: training loss 70.38, avg step time 40 ms\n","batch 1180: training loss 53.31, avg step time 40 ms\n","batch 1190: training loss 58.66, avg step time 40 ms\n","batch 1200: training loss 59.80, avg step time 40 ms\n","batch 1210: training loss 52.34, avg step time 40 ms\n","batch 1220: training loss 62.18, avg step time 40 ms\n","batch 1230: training loss 61.14, avg step time 40 ms\n","batch 1240: training loss 75.14, avg step time 40 ms\n","batch 1250: training loss 66.54, avg step time 40 ms\n","batch 1260: training loss 57.51, avg step time 40 ms\n","batch 1270: training loss 55.07, avg step time 40 ms\n","batch 1280: training loss 82.99, avg step time 40 ms\n","batch 1290: training loss 95.94, avg step time 40 ms\n","batch 1300: training loss 78.50, avg step time 40 ms\n","batch 1310: training loss 67.98, avg step time 40 ms\n","batch 1320: training loss 81.69, avg step time 40 ms\n","eval loss 163.67 (eval took 3442 ms)\n","batch 0: training loss 98.79, avg step time 42 ms\n","batch 10: training loss 79.57, avg step time 42 ms\n","batch 20: training loss 83.37, avg step time 41 ms\n","batch 30: training loss 63.96, avg step time 40 ms\n","batch 40: training loss 80.76, avg step time 39 ms\n","batch 50: training loss 61.02, avg step time 39 ms\n","batch 60: training loss 74.69, avg step time 39 ms\n","batch 70: training loss 60.29, avg step time 39 ms\n","batch 80: training loss 52.07, avg step time 39 ms\n","batch 90: training loss 56.70, avg step time 39 ms\n","batch 100: training loss 46.34, avg step time 39 ms\n","batch 110: training loss 61.25, avg step time 39 ms\n","batch 120: training loss 57.38, avg step time 39 ms\n","batch 130: training loss 54.31, avg step time 39 ms\n","batch 140: training loss 63.10, avg step time 39 ms\n","batch 150: training loss 76.54, avg step time 39 ms\n","batch 160: training loss 69.97, avg step time 40 ms\n","batch 170: training loss 75.36, avg step time 39 ms\n","batch 180: training loss 74.69, avg step time 39 ms\n","batch 190: training loss 67.92, avg step time 39 ms\n","batch 200: training loss 80.54, avg step time 39 ms\n","batch 210: training loss 59.76, avg step time 39 ms\n","batch 220: training loss 67.66, avg step time 39 ms\n","batch 230: training loss 56.51, avg step time 39 ms\n","batch 240: training loss 56.41, avg step time 39 ms\n","batch 250: training loss 68.60, avg step time 39 ms\n","batch 260: training loss 69.53, avg step time 39 ms\n","batch 270: training loss 73.24, avg step time 39 ms\n","batch 280: training loss 52.38, avg step time 39 ms\n","batch 290: training loss 73.55, avg step time 39 ms\n","batch 300: training loss 46.79, avg step time 39 ms\n","batch 310: training loss 47.65, avg step time 39 ms\n","batch 320: training loss 39.50, avg step time 39 ms\n","batch 330: training loss 66.09, avg step time 39 ms\n","batch 340: training loss 97.44, avg step time 39 ms\n","batch 350: training loss 64.03, avg step time 40 ms\n","batch 360: training loss 47.29, avg step time 40 ms\n","batch 370: training loss 82.70, avg step time 40 ms\n","batch 380: training loss 38.20, avg step time 40 ms\n","batch 390: training loss 79.88, avg step time 40 ms\n","batch 400: training loss 57.11, avg step time 40 ms\n","batch 410: training loss 51.69, avg step time 40 ms\n","batch 420: training loss 67.28, avg step time 40 ms\n","batch 430: training loss 49.73, avg step time 40 ms\n","batch 440: training loss 46.38, avg step time 40 ms\n","batch 450: training loss 62.01, avg step time 40 ms\n","batch 460: training loss 68.86, avg step time 40 ms\n","batch 470: training loss 61.20, avg step time 40 ms\n","batch 480: training loss 71.63, avg step time 40 ms\n","batch 490: training loss 79.74, avg step time 40 ms\n","batch 500: training loss 77.20, avg step time 40 ms\n","batch 510: training loss 58.33, avg step time 40 ms\n","batch 520: training loss 60.15, avg step time 40 ms\n","batch 530: training loss 48.75, avg step time 39 ms\n","batch 540: training loss 78.58, avg step time 39 ms\n","batch 550: training loss 38.66, avg step time 39 ms\n","batch 560: training loss 52.29, avg step time 39 ms\n","batch 570: training loss 39.21, avg step time 39 ms\n","batch 580: training loss 65.39, avg step time 39 ms\n","batch 590: training loss 80.22, avg step time 39 ms\n","batch 600: training loss 69.55, avg step time 39 ms\n","batch 610: training loss 59.51, avg step time 39 ms\n","batch 620: training loss 49.56, avg step time 39 ms\n","batch 630: training loss 70.73, avg step time 40 ms\n","batch 640: training loss 58.38, avg step time 40 ms\n","batch 650: training loss 42.97, avg step time 40 ms\n","batch 660: training loss 45.36, avg step time 40 ms\n","batch 670: training loss 39.42, avg step time 40 ms\n","batch 680: training loss 55.76, avg step time 39 ms\n","batch 690: training loss 58.88, avg step time 39 ms\n","batch 700: training loss 60.16, avg step time 39 ms\n","batch 710: training loss 47.05, avg step time 39 ms\n","batch 720: training loss 49.81, avg step time 39 ms\n","batch 730: training loss 48.93, avg step time 39 ms\n","batch 740: training loss 45.54, avg step time 40 ms\n","batch 750: training loss 44.21, avg step time 39 ms\n","batch 760: training loss 44.93, avg step time 39 ms\n","batch 770: training loss 47.10, avg step time 39 ms\n","batch 780: training loss 38.40, avg step time 39 ms\n","batch 790: training loss 57.63, avg step time 39 ms\n","batch 800: training loss 58.27, avg step time 40 ms\n","batch 810: training loss 51.71, avg step time 39 ms\n","batch 820: training loss 63.14, avg step time 40 ms\n","batch 830: training loss 51.44, avg step time 39 ms\n","batch 840: training loss 59.39, avg step time 39 ms\n","batch 850: training loss 47.94, avg step time 40 ms\n","batch 860: training loss 39.93, avg step time 40 ms\n","batch 870: training loss 33.80, avg step time 40 ms\n","batch 880: training loss 41.85, avg step time 40 ms\n","batch 890: training loss 84.50, avg step time 40 ms\n","batch 900: training loss 54.13, avg step time 40 ms\n","batch 910: training loss 69.68, avg step time 40 ms\n","batch 920: training loss 59.71, avg step time 40 ms\n","batch 930: training loss 98.14, avg step time 40 ms\n","batch 940: training loss 49.05, avg step time 40 ms\n","batch 950: training loss 70.62, avg step time 40 ms\n","batch 960: training loss 53.94, avg step time 40 ms\n","batch 970: training loss 59.09, avg step time 40 ms\n","batch 980: training loss 57.29, avg step time 40 ms\n","batch 990: training loss 60.99, avg step time 40 ms\n","batch 1000: training loss 55.09, avg step time 40 ms\n","batch 1010: training loss 67.17, avg step time 40 ms\n","batch 1020: training loss 50.95, avg step time 40 ms\n","batch 1030: training loss 56.53, avg step time 39 ms\n","batch 1040: training loss 37.07, avg step time 40 ms\n","batch 1050: training loss 53.55, avg step time 40 ms\n","batch 1060: training loss 38.79, avg step time 40 ms\n","batch 1070: training loss 32.97, avg step time 40 ms\n","batch 1080: training loss 42.44, avg step time 40 ms\n","batch 1090: training loss 46.69, avg step time 40 ms\n","batch 1100: training loss 49.24, avg step time 40 ms\n","batch 1110: training loss 62.18, avg step time 39 ms\n","batch 1120: training loss 55.00, avg step time 39 ms\n","batch 1130: training loss 47.40, avg step time 39 ms\n","batch 1140: training loss 65.26, avg step time 39 ms\n","batch 1150: training loss 28.37, avg step time 39 ms\n","batch 1160: training loss 47.17, avg step time 39 ms\n","batch 1170: training loss 53.65, avg step time 39 ms\n","batch 1180: training loss 41.67, avg step time 39 ms\n","batch 1190: training loss 44.13, avg step time 39 ms\n","batch 1200: training loss 44.21, avg step time 39 ms\n","batch 1210: training loss 40.95, avg step time 39 ms\n","batch 1220: training loss 46.23, avg step time 39 ms\n","batch 1230: training loss 46.30, avg step time 39 ms\n","batch 1240: training loss 55.96, avg step time 39 ms\n","batch 1250: training loss 51.18, avg step time 39 ms\n","batch 1260: training loss 44.17, avg step time 39 ms\n","batch 1270: training loss 42.21, avg step time 39 ms\n","batch 1280: training loss 62.34, avg step time 39 ms\n","batch 1290: training loss 70.91, avg step time 39 ms\n","batch 1300: training loss 55.25, avg step time 39 ms\n","batch 1310: training loss 51.52, avg step time 39 ms\n","batch 1320: training loss 64.17, avg step time 39 ms\n","eval loss 149.02 (eval took 3639 ms)\n","batch 0: training loss 78.07, avg step time 45 ms\n","batch 10: training loss 59.27, avg step time 38 ms\n","batch 20: training loss 64.43, avg step time 39 ms\n","batch 30: training loss 44.35, avg step time 40 ms\n","batch 40: training loss 58.92, avg step time 40 ms\n","batch 50: training loss 49.07, avg step time 40 ms\n","batch 60: training loss 55.41, avg step time 40 ms\n","batch 70: training loss 46.82, avg step time 41 ms\n","batch 80: training loss 38.95, avg step time 40 ms\n","batch 90: training loss 44.48, avg step time 40 ms\n","batch 100: training loss 35.45, avg step time 40 ms\n","batch 110: training loss 48.88, avg step time 40 ms\n","batch 120: training loss 41.73, avg step time 38 ms\n","batch 130: training loss 40.30, avg step time 37 ms\n","batch 140: training loss 47.42, avg step time 37 ms\n","batch 150: training loss 58.81, avg step time 36 ms\n","batch 160: training loss 53.55, avg step time 36 ms\n","batch 170: training loss 59.92, avg step time 37 ms\n","batch 180: training loss 59.71, avg step time 37 ms\n","batch 190: training loss 54.45, avg step time 38 ms\n","batch 200: training loss 61.46, avg step time 38 ms\n","batch 210: training loss 47.63, avg step time 38 ms\n","batch 220: training loss 52.60, avg step time 38 ms\n","batch 230: training loss 45.04, avg step time 38 ms\n","batch 240: training loss 43.09, avg step time 38 ms\n","batch 250: training loss 52.17, avg step time 38 ms\n","batch 260: training loss 53.52, avg step time 38 ms\n","batch 270: training loss 55.41, avg step time 38 ms\n","batch 280: training loss 40.84, avg step time 38 ms\n","batch 290: training loss 53.79, avg step time 39 ms\n","batch 300: training loss 36.57, avg step time 39 ms\n","batch 310: training loss 34.91, avg step time 39 ms\n","batch 320: training loss 31.82, avg step time 39 ms\n","batch 330: training loss 52.03, avg step time 39 ms\n","batch 340: training loss 73.53, avg step time 39 ms\n","batch 350: training loss 49.49, avg step time 39 ms\n","batch 360: training loss 36.84, avg step time 39 ms\n","batch 370: training loss 65.20, avg step time 39 ms\n","batch 380: training loss 30.24, avg step time 39 ms\n","batch 390: training loss 62.47, avg step time 39 ms\n","batch 400: training loss 43.06, avg step time 39 ms\n","batch 410: training loss 37.95, avg step time 39 ms\n","batch 420: training loss 50.75, avg step time 39 ms\n","batch 430: training loss 39.63, avg step time 39 ms\n","batch 440: training loss 35.72, avg step time 39 ms\n","batch 450: training loss 47.40, avg step time 39 ms\n","batch 460: training loss 53.63, avg step time 39 ms\n","batch 470: training loss 48.02, avg step time 39 ms\n","batch 480: training loss 56.66, avg step time 39 ms\n","batch 490: training loss 62.98, avg step time 39 ms\n","batch 500: training loss 63.32, avg step time 39 ms\n","batch 510: training loss 47.12, avg step time 39 ms\n","batch 520: training loss 46.01, avg step time 39 ms\n","batch 530: training loss 36.95, avg step time 39 ms\n","batch 540: training loss 63.61, avg step time 39 ms\n","batch 550: training loss 30.58, avg step time 39 ms\n","batch 560: training loss 41.35, avg step time 39 ms\n","batch 570: training loss 30.19, avg step time 39 ms\n","batch 580: training loss 53.59, avg step time 39 ms\n","batch 590: training loss 64.97, avg step time 39 ms\n","batch 600: training loss 54.29, avg step time 39 ms\n","batch 610: training loss 44.95, avg step time 39 ms\n","batch 620: training loss 39.19, avg step time 39 ms\n","batch 630: training loss 54.02, avg step time 39 ms\n","batch 640: training loss 44.54, avg step time 39 ms\n","batch 650: training loss 34.91, avg step time 39 ms\n","batch 660: training loss 37.00, avg step time 39 ms\n","batch 670: training loss 32.94, avg step time 39 ms\n","batch 680: training loss 42.43, avg step time 39 ms\n","batch 690: training loss 46.36, avg step time 39 ms\n","batch 700: training loss 48.94, avg step time 39 ms\n","batch 710: training loss 36.59, avg step time 39 ms\n","batch 720: training loss 41.22, avg step time 39 ms\n","batch 730: training loss 39.22, avg step time 39 ms\n","batch 740: training loss 36.55, avg step time 39 ms\n","batch 750: training loss 35.87, avg step time 39 ms\n","batch 760: training loss 37.68, avg step time 39 ms\n","batch 770: training loss 38.39, avg step time 39 ms\n","batch 780: training loss 29.69, avg step time 39 ms\n","batch 790: training loss 45.88, avg step time 39 ms\n","batch 800: training loss 47.73, avg step time 39 ms\n","batch 810: training loss 41.51, avg step time 39 ms\n","batch 820: training loss 48.92, avg step time 39 ms\n","batch 830: training loss 41.45, avg step time 39 ms\n","batch 840: training loss 49.23, avg step time 39 ms\n","batch 850: training loss 39.18, avg step time 39 ms\n","batch 860: training loss 32.21, avg step time 39 ms\n","batch 870: training loss 25.91, avg step time 39 ms\n","batch 880: training loss 33.33, avg step time 39 ms\n","batch 890: training loss 65.96, avg step time 39 ms\n","batch 900: training loss 43.35, avg step time 39 ms\n","batch 910: training loss 56.20, avg step time 39 ms\n","batch 920: training loss 47.46, avg step time 39 ms\n","batch 930: training loss 83.66, avg step time 39 ms\n","batch 940: training loss 41.68, avg step time 39 ms\n","batch 950: training loss 55.96, avg step time 39 ms\n","batch 960: training loss 42.21, avg step time 39 ms\n","batch 970: training loss 48.01, avg step time 39 ms\n","batch 980: training loss 47.86, avg step time 39 ms\n","batch 990: training loss 48.61, avg step time 39 ms\n","batch 1000: training loss 45.74, avg step time 39 ms\n","batch 1010: training loss 53.68, avg step time 39 ms\n","batch 1020: training loss 42.12, avg step time 39 ms\n","batch 1030: training loss 45.27, avg step time 39 ms\n","batch 1040: training loss 28.70, avg step time 39 ms\n","batch 1050: training loss 43.31, avg step time 39 ms\n","batch 1060: training loss 31.92, avg step time 39 ms\n","batch 1070: training loss 26.80, avg step time 39 ms\n","batch 1080: training loss 33.39, avg step time 39 ms\n","batch 1090: training loss 37.80, avg step time 39 ms\n","batch 1100: training loss 37.87, avg step time 39 ms\n","batch 1110: training loss 49.14, avg step time 39 ms\n","batch 1120: training loss 46.05, avg step time 39 ms\n","batch 1130: training loss 37.36, avg step time 39 ms\n","batch 1140: training loss 52.21, avg step time 39 ms\n","batch 1150: training loss 23.35, avg step time 39 ms\n","batch 1160: training loss 38.92, avg step time 39 ms\n","batch 1170: training loss 44.88, avg step time 39 ms\n","batch 1180: training loss 33.81, avg step time 40 ms\n","batch 1190: training loss 36.94, avg step time 39 ms\n","batch 1200: training loss 37.13, avg step time 39 ms\n","batch 1210: training loss 33.84, avg step time 40 ms\n","batch 1220: training loss 37.51, avg step time 40 ms\n","batch 1230: training loss 37.07, avg step time 40 ms\n","batch 1240: training loss 45.37, avg step time 40 ms\n","batch 1250: training loss 41.35, avg step time 40 ms\n","batch 1260: training loss 36.34, avg step time 40 ms\n","batch 1270: training loss 35.54, avg step time 40 ms\n","batch 1280: training loss 49.72, avg step time 40 ms\n","batch 1290: training loss 59.18, avg step time 40 ms\n","batch 1300: training loss 44.61, avg step time 40 ms\n","batch 1310: training loss 43.52, avg step time 40 ms\n","batch 1320: training loss 52.75, avg step time 40 ms\n","eval loss 144.12 (eval took 3603 ms)\n","batch 0: training loss 61.71, avg step time 41 ms\n","batch 10: training loss 48.63, avg step time 39 ms\n","batch 20: training loss 52.63, avg step time 39 ms\n","batch 30: training loss 35.72, avg step time 40 ms\n","batch 40: training loss 47.23, avg step time 40 ms\n","batch 50: training loss 39.97, avg step time 39 ms\n","batch 60: training loss 46.49, avg step time 39 ms\n","batch 70: training loss 37.29, avg step time 40 ms\n","batch 80: training loss 32.80, avg step time 40 ms\n","batch 90: training loss 35.16, avg step time 39 ms\n","batch 100: training loss 30.40, avg step time 39 ms\n","batch 110: training loss 42.85, avg step time 40 ms\n","batch 120: training loss 34.74, avg step time 40 ms\n","batch 130: training loss 32.37, avg step time 40 ms\n","batch 140: training loss 39.32, avg step time 40 ms\n","batch 150: training loss 48.99, avg step time 40 ms\n","batch 160: training loss 42.38, avg step time 40 ms\n","batch 170: training loss 49.16, avg step time 40 ms\n","batch 180: training loss 48.03, avg step time 40 ms\n","batch 190: training loss 44.00, avg step time 40 ms\n","batch 200: training loss 51.01, avg step time 40 ms\n","batch 210: training loss 40.81, avg step time 40 ms\n","batch 220: training loss 42.79, avg step time 40 ms\n","batch 230: training loss 39.68, avg step time 40 ms\n","batch 240: training loss 35.52, avg step time 40 ms\n","batch 250: training loss 44.11, avg step time 40 ms\n","batch 260: training loss 47.04, avg step time 40 ms\n","batch 270: training loss 47.03, avg step time 40 ms\n","batch 280: training loss 34.46, avg step time 40 ms\n","batch 290: training loss 45.15, avg step time 40 ms\n","batch 300: training loss 30.01, avg step time 40 ms\n","batch 310: training loss 27.39, avg step time 40 ms\n","batch 320: training loss 25.73, avg step time 40 ms\n","batch 330: training loss 42.15, avg step time 40 ms\n","batch 340: training loss 59.32, avg step time 40 ms\n","batch 350: training loss 40.92, avg step time 40 ms\n","batch 360: training loss 30.02, avg step time 40 ms\n","batch 370: training loss 53.09, avg step time 40 ms\n","batch 380: training loss 24.86, avg step time 40 ms\n","batch 390: training loss 50.88, avg step time 40 ms\n","batch 400: training loss 34.78, avg step time 40 ms\n","batch 410: training loss 29.89, avg step time 40 ms\n","batch 420: training loss 40.90, avg step time 40 ms\n","batch 430: training loss 33.24, avg step time 40 ms\n","batch 440: training loss 29.77, avg step time 40 ms\n","batch 450: training loss 39.45, avg step time 40 ms\n","batch 460: training loss 45.75, avg step time 40 ms\n","batch 470: training loss 39.33, avg step time 40 ms\n","batch 480: training loss 45.29, avg step time 40 ms\n","batch 490: training loss 52.28, avg step time 40 ms\n","batch 500: training loss 52.47, avg step time 40 ms\n","batch 510: training loss 39.03, avg step time 40 ms\n","batch 520: training loss 37.13, avg step time 40 ms\n","batch 530: training loss 31.54, avg step time 40 ms\n","batch 540: training loss 52.69, avg step time 40 ms\n","batch 550: training loss 26.19, avg step time 40 ms\n","batch 560: training loss 34.42, avg step time 40 ms\n","batch 570: training loss 25.97, avg step time 40 ms\n","batch 580: training loss 44.47, avg step time 40 ms\n","batch 590: training loss 54.35, avg step time 40 ms\n","batch 600: training loss 45.82, avg step time 40 ms\n","batch 610: training loss 36.78, avg step time 40 ms\n","batch 620: training loss 34.26, avg step time 40 ms\n","batch 630: training loss 43.83, avg step time 40 ms\n","batch 640: training loss 38.25, avg step time 40 ms\n","batch 650: training loss 29.99, avg step time 40 ms\n","batch 660: training loss 32.88, avg step time 40 ms\n","batch 670: training loss 27.22, avg step time 40 ms\n","batch 680: training loss 34.85, avg step time 40 ms\n","batch 690: training loss 39.77, avg step time 40 ms\n","batch 700: training loss 41.52, avg step time 40 ms\n","batch 710: training loss 31.09, avg step time 40 ms\n","batch 720: training loss 35.27, avg step time 40 ms\n","batch 730: training loss 33.88, avg step time 40 ms\n","batch 740: training loss 31.04, avg step time 40 ms\n","batch 750: training loss 30.36, avg step time 40 ms\n","batch 760: training loss 30.99, avg step time 40 ms\n","batch 770: training loss 33.07, avg step time 40 ms\n","batch 780: training loss 24.90, avg step time 40 ms\n","batch 790: training loss 38.96, avg step time 40 ms\n","batch 800: training loss 38.86, avg step time 40 ms\n","batch 810: training loss 36.32, avg step time 40 ms\n","batch 820: training loss 40.92, avg step time 40 ms\n","batch 830: training loss 35.61, avg step time 40 ms\n","batch 840: training loss 39.54, avg step time 40 ms\n","batch 850: training loss 32.78, avg step time 40 ms\n","batch 860: training loss 28.29, avg step time 40 ms\n","batch 870: training loss 22.18, avg step time 40 ms\n","batch 880: training loss 29.05, avg step time 40 ms\n","batch 890: training loss 54.98, avg step time 40 ms\n","batch 900: training loss 37.08, avg step time 40 ms\n","batch 910: training loss 46.04, avg step time 40 ms\n","batch 920: training loss 39.72, avg step time 40 ms\n","batch 930: training loss 69.75, avg step time 40 ms\n","batch 940: training loss 35.26, avg step time 40 ms\n","batch 950: training loss 48.78, avg step time 40 ms\n","batch 960: training loss 34.94, avg step time 40 ms\n","batch 970: training loss 41.07, avg step time 40 ms\n","batch 980: training loss 39.02, avg step time 40 ms\n","batch 990: training loss 41.50, avg step time 40 ms\n","batch 1000: training loss 39.45, avg step time 40 ms\n","batch 1010: training loss 47.47, avg step time 40 ms\n","batch 1020: training loss 35.78, avg step time 40 ms\n","batch 1030: training loss 38.67, avg step time 40 ms\n","batch 1040: training loss 24.77, avg step time 40 ms\n","batch 1050: training loss 36.36, avg step time 40 ms\n","batch 1060: training loss 27.23, avg step time 40 ms\n","batch 1070: training loss 23.34, avg step time 40 ms\n","batch 1080: training loss 31.94, avg step time 40 ms\n","batch 1090: training loss 32.85, avg step time 40 ms\n","batch 1100: training loss 32.17, avg step time 40 ms\n","batch 1110: training loss 42.17, avg step time 40 ms\n","batch 1120: training loss 39.20, avg step time 40 ms\n","batch 1130: training loss 33.44, avg step time 40 ms\n","batch 1140: training loss 44.72, avg step time 40 ms\n","batch 1150: training loss 20.33, avg step time 40 ms\n","batch 1160: training loss 33.01, avg step time 40 ms\n","batch 1170: training loss 37.27, avg step time 40 ms\n","batch 1180: training loss 30.13, avg step time 40 ms\n","batch 1190: training loss 32.82, avg step time 40 ms\n","batch 1200: training loss 31.57, avg step time 40 ms\n","batch 1210: training loss 29.51, avg step time 39 ms\n","batch 1220: training loss 33.40, avg step time 39 ms\n","batch 1230: training loss 30.72, avg step time 39 ms\n","batch 1240: training loss 38.27, avg step time 39 ms\n","batch 1250: training loss 33.85, avg step time 39 ms\n","batch 1260: training loss 31.74, avg step time 39 ms\n","batch 1270: training loss 30.29, avg step time 39 ms\n","batch 1280: training loss 42.59, avg step time 39 ms\n","batch 1290: training loss 50.80, avg step time 39 ms\n","batch 1300: training loss 38.03, avg step time 39 ms\n","batch 1310: training loss 36.93, avg step time 39 ms\n","batch 1320: training loss 45.33, avg step time 39 ms\n","eval loss 143.46 (eval took 3539 ms)\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]}]}